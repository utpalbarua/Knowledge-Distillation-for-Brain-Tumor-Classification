# ğŸ§  Brain Tumor MRI Classification using Knowledge Distillation

A PyTorch implementation of **knowledge distillation** for brain tumor classification, where a lightweight **MobileNetV3-Small** student model learns from a powerful **ResNet-101** teacher model.

---

## ğŸ“‹ Overview

This project demonstrates how knowledge distillation can compress a large neural network into a smaller, more efficient model while maintaining high accuracy.
The system classifies brain MRI images into four categories:

* Glioma
* Meningioma
* No Tumor
* Pituitary Tumor

### âœ¨ Key Features

* ğŸ§  **Teacher Model**: ResNet-101 (pre-trained on ImageNet)
* ğŸ“± **Student Model**: MobileNetV3-Small (pre-trained on ImageNet)
* ğŸ”¥ **Knowledge Distillation**: Temperature-scaled soft targets + hard loss
* ğŸ¨ **Visualization**: Grad-CAM heatmaps to compare teacher vs student attention
* ğŸ“ˆ **High Performance**: Student achieves **98.7%** accuracy on the test set

---

## ğŸ¯ Results

| Model                       | Accuracy | F1 Score |
| --------------------------- | -------- | -------- |
| Teacher (ResNet-101)        | 99.30%   | -        |
| Student (MobileNetV3-Small) | 98.70%   | 0.986    |

### ğŸ“‰ Confusion Matrix

|                | Glioma | Meningioma | NoTumor | Pituitary |
| -------------- | ------ | ---------- | ------- | --------- |
| **Glioma**     | 288    | 11         | 0       | 1         |
| **Meningioma** | 1      | 303        | 2       | 0         |
| **NoTumor**    | 0      | 0          | 405     | 0         |
| **Pituitary**  | 0      | 2          | 0       | 298       |

---

## ğŸš€ Getting Started

### ğŸ“¦ Prerequisites

```bash
pip install torch torchvision
pip install scikit-learn matplotlib seaborn tqdm grad-cam
```

---

### ğŸ“ Dataset Structure

```plaintext
brain-tumor-mri-dataset/
â”œâ”€â”€ Training/
â”‚   â”œâ”€â”€ glioma/
â”‚   â”œâ”€â”€ meningioma/
â”‚   â”œâ”€â”€ notumor/
â”‚   â””â”€â”€ pituitary/
â””â”€â”€ Testing/
    â”œâ”€â”€ glioma/
    â”œâ”€â”€ meningioma/
    â”œâ”€â”€ notumor/
    â””â”€â”€ pituitary/
```

---

## ğŸ—ï¸ Training

### Train the Teacher Model:

```bash
python train_teacher(epochs=10)
```

### Train the Student Model with Knowledge Distillation:

```bash
python train_student(epochs=10)
```

### Evaluate the Student Model:

```bash
python evaluate(student, test_loader)
```

---

## ğŸ”¬ Methodology

### ğŸ“˜ Knowledge Distillation Loss

The distillation loss combines **hard** and **soft** components:

```python
loss = Î± * hard_loss + (1 - Î±) * soft_loss
```

* **Hard Loss**: Cross-entropy with true labels
* **Soft Loss**: KL divergence between student and teacher predictions
* **Temperature (T=4)**: Softens probability distributions
* **Alpha (Î±=0.5)**: Balances hard and soft losses

---

### ğŸ§ª Data Augmentation

* Random horizontal flip
* Random rotation (Â±15Â°)
* Resize to 224Ã—224
* ImageNet normalization

---

## ğŸ“Š Visualizations

The project includes visualization tools to better understand model performance:

* ğŸ“ˆ Prediction Comparison: Bar charts for teacher vs student confidence
* ğŸ“‰ Confusion Matrix: Heatmap of classification performance
* ğŸ”¥ Grad-CAM: Visualizes attention regions in MRI scans

Example usage:

```python
visualize_classification_kd(teacher, student, val_dataset, idx=1000, device='cuda')
```

---

## ğŸ—ï¸ Architecture Details

### ğŸ§  Teacher Network (ResNet-101)

* Depth: 101 layers
* Parameters: ~44M
* Final layer modified for 4-class classification

### ğŸ“± Student Network (MobileNetV3-Small)

* Lightweight architecture
* Parameters: ~2.5M (**94% reduction**)
* Final layer modified for 4-class classification

---

## ğŸ“ˆ Training Configuration

```python
Batch Size: 32
Learning Rate: 1e-4
Optimizer: Adam
Teacher Epochs: 10
Student Epochs: 10
Validation Split: 20%
```

---

## ğŸ” Key Functions

* `train_teacher()` â€“ Train the ResNet-101 teacher model
* `train_student()` â€“ Train the student using KD
* `kd_loss()` â€“ Compute combined distillation loss
* `evaluate()` â€“ Calculate model accuracy
* `visualize_classification_kd()` â€“ Visualize predictions with Grad-CAM

---

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
```

---

## ğŸ“„ License

This project is available for **educational and research purposes** under the MIT License.

---

## ğŸ™ Acknowledgments

* ğŸ§¬ Brain Tumor MRI Dataset from [Kaggle](https://www.kaggle.com/)
* ğŸ§  PyTorch and torchvision teams
* ğŸ”¥ Grad-CAM implementation from [pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam)

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!
Feel free to check the [issues page](../../issues).
